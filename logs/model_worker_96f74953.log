2024-05-30 19:42:42 | INFO | model_worker | Loading the model ['chatglm3-6b'] on worker 96f74953 ...
2024-05-30 19:42:43 | ERROR | stderr | Loading checkpoint shards:   0%|                                                                                                                            | 0/7 [00:00<?, ?it/s]
2024-05-30 19:42:44 | ERROR | stderr | Loading checkpoint shards:  14%|████████████████▌                                                                                                   | 1/7 [00:01<00:09,  1.59s/it]
2024-05-30 19:42:46 | ERROR | stderr | Loading checkpoint shards:  29%|█████████████████████████████████▏                                                                                  | 2/7 [00:03<00:08,  1.66s/it]
2024-05-30 19:42:48 | ERROR | stderr | Loading checkpoint shards:  43%|█████████████████████████████████████████████████▋                                                                  | 3/7 [00:04<00:06,  1.65s/it]
2024-05-30 19:42:49 | ERROR | stderr | Loading checkpoint shards:  57%|██████████████████████████████████████████████████████████████████▎                                                 | 4/7 [00:06<00:04,  1.63s/it]
2024-05-30 19:42:51 | ERROR | stderr | Loading checkpoint shards:  71%|██████████████████████████████████████████████████████████████████████████████████▊                                 | 5/7 [00:08<00:03,  1.66s/it]
2024-05-30 19:42:53 | ERROR | stderr | Loading checkpoint shards:  86%|███████████████████████████████████████████████████████████████████████████████████████████████████▍                | 6/7 [00:09<00:01,  1.68s/it]
2024-05-30 19:42:54 | ERROR | stderr | Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:11<00:00,  1.47s/it]
2024-05-30 19:42:54 | ERROR | stderr | Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:11<00:00,  1.57s/it]
2024-05-30 19:42:54 | ERROR | stderr | 
2024-05-30 19:42:56 | ERROR | stderr | Process model_worker - chatglm3-6b:
2024-05-30 19:42:56 | ERROR | stderr | Traceback (most recent call last):
2024-05-30 19:42:56 | ERROR | stderr |   File "/root/pyenv/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
2024-05-30 19:42:56 | ERROR | stderr |     self.run()
2024-05-30 19:42:56 | ERROR | stderr |   File "/root/pyenv/lib/python3.10/multiprocessing/process.py", line 108, in run
2024-05-30 19:42:56 | ERROR | stderr |     self._target(*self._args, **self._kwargs)
2024-05-30 19:42:56 | ERROR | stderr |   File "/root/Langchain-Chatchat/startup.py", line 388, in run_model_worker
2024-05-30 19:42:56 | ERROR | stderr |     app = create_model_worker_app(log_level=log_level, **kwargs)
2024-05-30 19:42:56 | ERROR | stderr |   File "/root/Langchain-Chatchat/startup.py", line 216, in create_model_worker_app
2024-05-30 19:42:56 | ERROR | stderr |     worker = ModelWorker(
2024-05-30 19:42:56 | ERROR | stderr |   File "/root/pyenv/lib/python3.10/site-packages/fastchat/serve/model_worker.py", line 77, in __init__
2024-05-30 19:42:56 | ERROR | stderr |     self.model, self.tokenizer = load_model(
2024-05-30 19:42:56 | ERROR | stderr |   File "/root/pyenv/lib/python3.10/site-packages/fastchat/model/model_adapter.py", line 362, in load_model
2024-05-30 19:42:56 | ERROR | stderr |     model.to(device)
2024-05-30 19:42:56 | ERROR | stderr |   File "/root/pyenv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2460, in to
2024-05-30 19:42:56 | ERROR | stderr |     return super().to(*args, **kwargs)
2024-05-30 19:42:56 | ERROR | stderr |   File "/root/pyenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in to
2024-05-30 19:42:56 | ERROR | stderr |     return self._apply(convert)
2024-05-30 19:42:56 | ERROR | stderr |   File "/root/pyenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
2024-05-30 19:42:56 | ERROR | stderr |     module._apply(fn)
2024-05-30 19:42:56 | ERROR | stderr |   File "/root/pyenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
2024-05-30 19:42:56 | ERROR | stderr |     module._apply(fn)
2024-05-30 19:42:56 | ERROR | stderr |   File "/root/pyenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 810, in _apply
2024-05-30 19:42:56 | ERROR | stderr |     module._apply(fn)
2024-05-30 19:42:56 | ERROR | stderr |   [Previous line repeated 3 more times]
2024-05-30 19:42:56 | ERROR | stderr |   File "/root/pyenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 833, in _apply
2024-05-30 19:42:56 | ERROR | stderr |     param_applied = fn(param)
2024-05-30 19:42:56 | ERROR | stderr |   File "/root/pyenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1158, in convert
2024-05-30 19:42:56 | ERROR | stderr |     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2024-05-30 19:42:56 | ERROR | stderr | torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 214.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 191.56 MiB is free. Process 819123 has 12.44 GiB memory in use. Process 819960 has 1.69 GiB memory in use. Process 979945 has 9.32 GiB memory in use. Of the allocated memory 8.94 GiB is allocated by PyTorch, and 1.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
